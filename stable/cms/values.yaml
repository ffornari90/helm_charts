htcondor:
  ttsCache:
    enabled: true
    image: ghcr.io/dodas-ts/tts-cache
    tag: v1.0.1
    iamToken: TOKEN_DUMMY
    iamClientId: ID_DUMMY
    iamClientSecret: SECRET_DUMMY
  schedd:
    enabled: false
  master:
    enabled: false
  squid:
    enabled: true
    image: dodasts/squid
    tag: v1.1.2-dodas
  cvmfs:
    enabled: true 
    image: dodasts/cvmfs
    tag: v1.4-reloaded
    pullPolicy: IfNotPresent
    replicas: 1
    # # List of repos to be mounted
    repoList: cms.cern.ch  grid.cern.ch
    defaultLocalConfig:
      - file: cms.cern.ch.conf
        content: |
          export CMS_LOCAL_SITE=/etc/cvmfs/SITECONF
          CVMFS_HTTP_PROXY=http://localhost:3128
      - file: grid.cern.ch.conf
        content: \"CVMFS_HTTP_PROXY=http://localhost:3128\"
  wn:
    # Condor slot type
    slotType: cpus=1, mem=2000
    requests:
      memory: "1500M"
      cpu: 1
    replicas: 1
    image:
      name: dodasts/cms
      tag: latest
    args: /usr/local/bin/dodas.sh
    siteConfCMS:
      enabled: true
      numCPUS: 1
      files:
        - name: sitelocal
          path: JobConfig
          filename: site-local-config.xml
          content: |
            <site-local-config>
            <site name=\"{{cms_config_cms_local_site}}\">
            <event-data>
            <catalog url=\"trivialcatalog_file:/cvmfs/cms.cern.ch/SITECONF/local/PhEDEx/storage.xml?protocol={{cms_input_protocol}}\"/>
            </event-data>
            <calib-data>
            <frontier-connect>
            <load balance=\"proxies\"/>
            <proxy url=\"http://localhost:3128\"/>
            <backupproxy url=\"http://cmsbpfrontier.cern.ch:3128\"/>
            <backupproxy url=\"http://cmsbproxy.fnal.gov:3128\"/>
            <server url=\"http://cmsfrontier.cern.ch:8000/FrontierInt\"/>
            <server url=\"http://cmsfrontier1.cern.ch:8000/FrontierInt\"/>
            <server url=\"http://cmsfrontier2.cern.ch:8000/FrontierInt\"/>
            <server url=\"http://cmsfrontier3.cern.ch:8000/FrontierInt\"/>
            </frontier-connect>
            </calib-data>
            <local-stage-out>
              <command value=\"{{cms_config_stageoutcommand}}\"/>
              <catalog url=\"trivialcatalog_file:/cvmfs/cms.cern.ch/SITECONF/{{cms_config_stageoutsite}}/PhEDEx/storage.xml?protocol={{cms_config_stageoutprotocol}}\"/>
              <se-name value=\"srm-eoscms.cern.ch\"/>
              <phedex-node value=\"{{cms_config_phedexnode}}\"/>
            </local-stage-out>
            <fallback-stage-out>
              <se-name value=\"t2-srm-02.lnl.infn.it\"/>
              <phedex-node value=\"{{cms_config_fallback_phedexnode}}\"/>
              <lfn-prefix value=\"{{cms_config_fallback_lfn_prefix}}\"/>
              <command value=\"{{cms_config_fallback_command}}\"/>
            </fallback-stage-out>
            </site>
            </site-local-config>
        - name: storage
          path: PhEDEx
          filename: storage.xml
          content: |
            <storage-mapping>
            <!-- AAA xrootd read rule -->
            <lfn-to-pfn protocol=\"xrootd\"
                    destination-match=\".*\"
                    path-match=\"/+store/(.*)\"
                    result=\"root://{{cms_xrd_readserver}}//store/$1\"/>
            <!-- Onedata read rule -->
            <lfn-to-pfn protocol=\"onedata\"
                    destination-match=\".*\"
                    path-match=\"/(.*)\"
                    result=\"/mnt/onedata/{{cms_input_path}}/$1\"/>
            </storage-mapping>
    config: |
      01_DODAS_Global: |
        # Defaults to root@$(FULL_HOSTNAME)
        #CONDOR_ADMIN = < some valid email >
        ## The condor ids may not be necessary
        #CONDOR_IDS = < uid.gid >

        CONDOR_HOST1=vocms0115.cern.ch
        CONDOR_HOST=$(CONDOR_HOST1)

        COLLECTOR_HOST1=$(CONDOR_HOST1):9618
        COLLECTOR_HOST=$(COLLECTOR_HOST1)

        # CCB to contact for all communication
        CCB_ADDRESS = $(CONDOR_HOST)

        # Collector to talk to
        COLLECTOR_NAME        =
        COLLECTOR_HOST        = $(CONDOR_HOST)
        MASTER.COLLECTOR_HOST = $(CONDOR_HOST)

        UID_DOMAIN = $(HOSTNAME)
        FILESYSTEM_DOMAIN = $(HOSTNAME)

        # How and when to send mail
        MAIL               = /bin/mail
        PUBLISH_OBITUARIES = False

        # Do not delegate any GSI credentials, usually this causes problems
        DELEGATE_JOB_GSI_CREDENTIALS = False

        # Up the max file descriptors so we do not get unusual errors
        #MAX_FILE_DESCRIPTORS = 131072

        # On updates, do a peaceful restart to preserve running jobs.
        MASTER_NEW_BINARY_RESTART = PEACEFUL

        ## Disable VOMS checking
        USE_VOMS_ATTRIBUTES = False

        # LSB Attributes
        LSB_DISTRIBUTOR_ID = "ScientificFermi"
        LSB_RELEASE        = "6.5"
        LSB_DESCRIPTION    = "Scientific Linux Fermi release 6.5 (Ramsey)"
      02_DODAS_Worker_Defaults: |
        DAEMON_LIST = MASTER,STARTD

        # Set the execute working directory
        EXECUTE = /home/condor

        # job wrapper location

        #USER_JOB_WRAPPER = /usr/libexec/condor_job_wrapper.sh 

        # Set the default environment for the job
        STARTER_JOB_ENVIRONMENT = "LD_LIBRARY_PATH=/usr/lib 'VO_CMS_SW_DIR=/cvmfs/cms.cern.ch' _JAVA_OPTIONS=-Xmx1024m"
        JOB_INHERITS_STARTER_ENVIRONMENT = True

        # RANK is now set by a special gwms config file.  This config file is 
        # auto generated by querying the factory collector for the frontend
        # supplied attributes.  RANK and START are both set by this script
        # RANK = < Do we want a RANK expression?  If none given, it defaults to 0.0 >

        # Including this here so that we can make the distinction between HLT/Tier 0 slots
        # and "normal" glideins.  Both will be in the pool, but the HLT can only run
        # certain types of jobs and we may want to restrict the type of jobs that are
        # run on the Tier 0
        IS_GLIDEIN = False

        # include the following attributes in the STARTD and SLOT classads
        STARTD_ATTRS = IS_GLIDEIN, START, LSB_RELEASE, LSB_DISTRIBUTOR_ID, LSB_DESCRIPTION
        STARTD_SLOT_ATTRS = State, Activity, EnteredCurrentActivity, TotalTimeUnclaimedIdle, TotalTimeClaimedBusy

        # configure confor_preen
        PREEN_ARGS              = -r
        PREEMPTION_REQUIREMENTS = False

        # Job behaviors
        CONTINUE                = True
        PREEMPT                 = False
        START                   = True
        SUSPEND                 = False
        SUSPEND_VANILLA         = False
        WANT_SUSPEND            = False
        WANT_SUSPEND_VANILLA    = False
        WANT_UDP_COMMAND_SOCKET = False
        WANT_VACATE             = True

        # if it ever enters the Preempting/Vacating state, get out of it in 5 minutes
        # should never happen, but it is a good precaution
        KILL = (CurrentTime - EnteredCurrentActivity > 300)

        # Some possibly useful statistics
        StateTimer      = (CurrentTime - EnteredCurrentState)
        ActivityTimer   = (CurrentTime - EnteredCurrentActivity)
        ActivationTimer = (CurrentTime - JobStart)

        STARTER_UPDATE_INTERVAL = 60

        # randomize the TCP updates to the collector so we don't swamp the collector all
        # at once
        UPDATE_COLLECTOR_WITH_TCP = True
        UPDATE_INTERVAL           = $RANDOM_INTEGER(270, 370, 1)
        MASTER_UPDATE_INTERVAL    = $RANDOM_INTEGER(270, 330, 1)

        # Set Logging
        STARTD_HISTORY  = $(LOG)/StartdHistoryLog
        MAX_MASTER_LOG  = 1000000
        MAX_STARTD_LOG  = 10000000
        MAX_STARTER_LOG = 10000000

      03_DODAS_Partitionable_slots: |
        NUM_CPUS                  = $(DETECTED_CORES)
        NUM_SLOTS                 = 1
        NUM_SLOTS_TYPE_1          = 1
        SLOT_TYPE_1               = 100%
        SLOT_TYPE_1_PARTITIONABLE = True

      04_DODAS_Security: |
        USE_PROCESS_GROUPS = False

        DENY_WRITE         = anonymous@*
        DENY_ADMINISTRATOR = anonymous@*
        DENY_DAEMON        = anonymous@*
        DENY_NEGOTIATOR    = anonymous@*

        SEC_DEFAULT_AUTHENTICATION                = REQUIRED
        SEC_DEFAULT_AUTHENTICATION_METHODS        = GSI 
        SEC_DEFAULT_ENCRYPTION                    = OPTIONAL
        SEC_DEFAULT_INTEGRITY                     = REQUIRED
        SEC_DEFAULT_SESSION_DURATION              = 337623
        SEC_ENABLE_MATCH_PASSWORD_AUTHENTICATION  = True

        GSI_DELEGATION_KEYBITS    = 1024
        GSI_DAEMON_NAME = /DC=ch/DC=cern/OU=computers/CN=vocms0115.cern.ch
        # The CERN frontend details
        GSI_DAEMON_NAME=$(GSI_DAEMON_NAME),/DC=ch/DC=cern/OU=computers/CN=frontend02/vocms080.cern.ch
        GSI_DAEMON_NAME=$(GSI_DAEMON_NAME),/DC=ch/DC=cern/OU=computers/CN=cmspilot02/vocms080.cern.ch
        # FNAL frontend details (as HA)
        GSI_DAEMON_NAME=$(GSI_DAEMON_NAME),/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=cmsgwms-frontend.fnal.gov
        GSI_DAEMON_NAME=$(GSI_DAEMON_NAME),/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=pilot/cmsgwms-frontend.fnal.gov
        GSI_DAEMON_TRUSTED_CA_DIR = /cvmfs/grid.cern.ch/etc/grid-security/certificates
        ## HLT admins expressed interest in using a host cert baked into the VM image
        #GSI_DAEMON_CERT           = /etc/grid-security/certificates/hostcert.pem
        ## Tier 0 admins wanted to use a proxy baked into the VM image
        GSI_DAEMON_PROXY          = /root/proxy/gwms_proxy

        CERTIFICATE_MAPFILE       = /etc/condor/certs/condor_mapfile

        # Extras  
        ALLOW_DAEMON = $(ALLOW_DAEMON), submit-side@matchsession/*.fnal.gov
        ALLOW_DAEMON = $(ALLOW_DAEMON), 172.17.0.2/24
        ALLOW_DAEMON = $(ALLOW_DAEMON), submit-side@matchsession/*.cern.ch

        USE_CCB = "True"

      05_DODAS_Debugging_settings: |
        ALL_DEBUG     = D_PID
        STARTD_DEBUG  = D_PID D_JOB
        STARTER_DEBUG = D_PID 
        MASTER_DEBUG  = D_PID

      99_DODAS_local: |
        # This file is AUTO-GENERATED and will be overwritten.
        GLIDEIN_Site = "T3_IT_Opportunistic" 
        GLIDEIN_In_Downtime = "True"
        GLIDEIN_Gatekeeper = "T3_IT_Opportunistic:8443"
        GLIDEIN_Supported_VOs = "CMS"
        START = ifthenelse(DESIRED_Sites isnt undefined, stringListMember(GLIDEIN_CMSSite,DESIRED_Sites), undefined)
        GLIDEIN_Req_MUPJ_gLExec = "False"
        GLIDEIN_REQUIRE_GLEXEC_USE = "False"
        GLIDEIN_MaxMemMBs = 28000
        GLIDEIN_Max_Walltime = 2600000
        GLIDEIN_Max_Idle = 2600000
        GLIDEIN_Retire_Time = 2427000
        GLIDEIN_REQUIRED_OS = "rhel6"
        GLIDEIN_SlotsLayout = "fixed"
        GLIDEIN_GridType = "ec2"
        GLIDEIN_REQUIRE_VOMS = "False"
        GLIDEIN_CMSSite = "T3_IT_Opportunistic" 
        GLIDEIN_SupportedAuthenticationMethod = "key_pair"
        GLIDEIN_Downtime_Comment = "On 2015-03-11 set to indefinite downtime suggested by Farrukh. --Vassil"
        GLIDEIN_Retire_Time_Spread = 7200
        GLIDEIN_TrustDomain = "HLT_Grizzly"
        GLIDEIN_GlobusRSL = ""
        STARTD_ATTRS = $(STARTD_ATTRS) GLIDEIN_Site GLIDEIN_In_Downtime GLIDEIN_Gatekeeper GLIDEIN_Supported_VOs GLIDEIN_Req_MUPJ_gLExec GLIDEIN_REQUIRE_GLEXEC_USE GLIDEIN_MaxMemMBs GLIDEIN_Max_Walltime GLIDEIN_Max_Idle GLIDEIN_Retire_Time GLIDEIN_REQUIRED_OS GLIDEIN_SlotsLayout GLIDEIN_GridType GLIDEIN_REQUIRE_VOMS GLIDEIN_CMSSite GLIDEIN_SupportedAuthenticationMethod GLIDEIN_Downtime_Comment GLIDEIN_Retire_Time_Spread GLIDEIN_TrustDomain GLIDEIN_GlobusRSL
        #GSI_DAEMON_NAME = "\/DC\=ch\/DC\=cern\/OU\=computers\/CN\=vocms0115\.cern\.ch" 
        STARTER_ALLOW_RUNAS_OWNER = False
        STARTER_DEBUG = D_ALL

      99_DODAS_tweaks: |
        # List of collectors
        GSI_DAEMON_NAME=$(GSI_DAEMON_NAME),/DC=ch/DC=cern/OU=computers/CN=vocms0815.cern.ch
        GSI_DAEMON_NAME=$(GSI_DAEMON_NAME),/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=cmssrv221.fnal.gov

        # Global pool CCBs
        GSI_DAEMON_NAME=$(GSI_DAEMON_NAME),/DC=ch/DC=cern/OU=computers/CN=vocms0806.cern.ch
        GSI_DAEMON_NAME=$(GSI_DAEMON_NAME),/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=cmssrv258.fnal.gov

        # in case of HA setup, use:
        #COLLECTOR_HOST=cmsgwms-collector-global.cern.ch:COLLECTOR_PORT,cmssrv221.fnal.gov:COLLECTOR_PORT
        #CCB_ADDRESS=cmsgwms-ccb-global.cern.ch:CCB_PORT,cmssrv258.fnal.gov:CCB_PORT
        COLLECTOR_HOST=cmsgwms-collector-global.cern.ch:COLLECTOR_PORT
        COLLECTOR_HOST1=$(COLLECTOR_HOST)
        CCB_ADDRESS=cmsgwms-ccb-global.cern.ch:CCB_PORT

        MUST_MODIFY_REQUEST_EXPRS = False
        Requirements = START

        # Indicates that the startd can run any OS jobs
        GLIDEIN_REQUIRED_OS = "any"
        # Indicates that the startd has singularity
        HAS_Singularity=True
        # Indicates what the default singularity image should be (= CMS RHEL6 image)
        OSG_SINGULARITY_IMAGE_DEFAULT="/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6:latest"
        # Indicates where to find the singularity binary
        OSG_SINGULARITY_PATH="/usr/bin/singularity"
        # Indicates the version of singularity
        OSG_SINGULARITY_VERSION="2.4.2-dist"
        # Tells the startd to advertise these attributes to the central manager
        STARTD_ATTRS = $(STARTD_ATTRS) GLIDEIN_REQUIRED_OS HAS_Singularity OSG_SINGULARITY_VERSION OSG_SINGULARITY_PATH OSG_SINGULARITY_IMAGE_DEFAULT

        USER_JOB_WRAPPER=/usr/local/libexec/singularity_wrapper.sh
